{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation - 2nd Challenge\n",
    "\n",
    "## Pretrained encoder (Resnet50), UNet architecture\n",
    "\n",
    "In this notebook we create an architecture based on UNet, importing a pretrained model as the encoder and the first convolution of VGG16.\n",
    "\n",
    "We performed some trials with VGG16, VGG19 and Resnet. Resnet performs better than others as the encoder part.\n",
    "\n",
    "\n",
    "Main aspects:\n",
    "- Skip connections between the encoder and the decoder\n",
    "- Finetuning the overall encoder starting from the pretrained weights (from imagenet) using a small learning rate (1e-4)\n",
    "- Dice loss implementation\n",
    "- Last layer of encoder formed by a concatenation between resnet, VGG16 and previous layers of the encoder\n",
    "\n",
    "\n",
    "As a reference for this implementation, we have used this repo:\n",
    "https://github.com/killthekitten/kaggle-carvana-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "apply_data_augmentation = True\n",
    "\n",
    "\n",
    "if apply_data_augmentation:\n",
    "    train_img_data_gen = ImageDataGenerator(featurewise_center=True,\n",
    "                                            featurewise_std_normalization=True,\n",
    "                                            horizontal_flip=True,\n",
    "                                            vertical_flip=True,\n",
    "                                            validation_split=0.2,\n",
    "                                            rescale=1./255,\n",
    "                                            fill_mode=\"reflect\")\n",
    "    train_mask_data_gen = ImageDataGenerator(featurewise_center=True,\n",
    "                                            featurewise_std_normalization=True,\n",
    "                                            horizontal_flip=True,\n",
    "                                            vertical_flip=True,\n",
    "                                            rescale=1./255,\n",
    "                                            fill_mode=\"reflect\",\n",
    "                                            validation_split=0.2)\n",
    "else:\n",
    "    train_img_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    train_mask_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset\"\n",
    "\n",
    "bs = 4\n",
    "\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "\n",
    "#------------------- TRAINING DATASET ---------------------\n",
    "\n",
    "training_dir = os.path.join(dataset_dir, 'training')\n",
    "train_img_gen = train_img_data_gen.flow_from_directory(os.path.join(training_dir, 'images'),\n",
    "                                                       target_size=(img_h, img_w),\n",
    "                                                       batch_size=bs, \n",
    "                                                       class_mode=None,\n",
    "                                                       shuffle=True,\n",
    "                                                       interpolation='bilinear',\n",
    "                                                       seed=SEED,\n",
    "                                                       color_mode = 'rgb',\n",
    "                                                       subset='training')  \n",
    "train_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'masks'),\n",
    "                                                         target_size=(img_h, img_w),\n",
    "                                                         batch_size=bs,\n",
    "                                                         class_mode=None,\n",
    "                                                         shuffle=True,\n",
    "                                                         interpolation='bilinear',\n",
    "                                                         seed=SEED,\n",
    "                                                         color_mode='grayscale',\n",
    "                                                         subset='training')\n",
    "train_gen = zip(train_img_gen, train_mask_gen)\n",
    "\n",
    "\n",
    "\n",
    "#------------------- VALIDATION DATASET ---------------------\n",
    "\n",
    "valid_img_gen = train_img_data_gen.flow_from_directory(os.path.join(training_dir, 'images'),\n",
    "                                                       target_size=(img_h, img_w),\n",
    "                                                       batch_size=bs, \n",
    "                                                       class_mode=None,\n",
    "                                                       shuffle=False,\n",
    "                                                       interpolation='bilinear',\n",
    "                                                       seed=SEED,\n",
    "                                                       color_mode = 'rgb',\n",
    "                                                       subset='validation')\n",
    "valid_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'masks'),\n",
    "                                                         target_size=(img_h, img_w),\n",
    "                                                         batch_size=bs, \n",
    "                                                         class_mode=None,\n",
    "                                                         shuffle=False,\n",
    "                                                         interpolation='bilinear',\n",
    "                                                         seed=SEED,\n",
    "                                                         color_mode='grayscale',\n",
    "                                                         subset='validation')\n",
    "valid_gen = zip(valid_img_gen, valid_mask_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target(x_, y_):\n",
    "    y_ = tf.cast(y_, tf.int32)\n",
    "    return x_, y_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------- TRAIN DATASET ---------------------\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_target)\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------- VALIDATION DATASET ---------------------\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "\n",
    "valid_dataset = valid_dataset.map(prepare_target)\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Activation, SpatialDropout2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(prevlayer, filters, prefix, strides=(1, 1)):\n",
    "    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = BatchNormalization(name=prefix + \"_bn\")(conv)\n",
    "    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def resnetUnet(resnet,input_shape):\n",
    "    \n",
    "    #Set trainable layers in the encoder --> [small learning rate]\n",
    "    for l in resnet.layers:\n",
    "        l.trainable = True\n",
    "        \n",
    "        \n",
    "    # Get layers from resnet to add skip connections between encoder and decoder\n",
    "    conv1 = resnet.get_layer('conv1_relu').output\n",
    "    conv2 = resnet.get_layer('conv2_block3_out').output\n",
    "    conv3 = resnet.get_layer('conv3_block4_out').output\n",
    "    conv4 = resnet.get_layer('conv4_block6_out').output\n",
    "    conv5 = resnet.get_layer('conv5_block3_out').output\n",
    "    \n",
    "    \n",
    "    \n",
    "    #     ------ DECODER  ------\n",
    "    \n",
    "    # -- 1 --\n",
    "    up6 = concatenate([UpSampling2D()(conv5), conv4], axis=-1)\n",
    "    conv6 = conv_block(up6, 256, \"conv6_1\")\n",
    "    conv6 = conv_block(conv6, 256, \"conv6_2\")\n",
    "    \n",
    "    # -- 2 --\n",
    "    up7 = concatenate([UpSampling2D()(conv6), conv3], axis=-1)\n",
    "    conv7 = conv_block(up7, 192, \"conv7_1\")\n",
    "    conv7 = conv_block(conv7, 192, \"conv7_2\")\n",
    "    \n",
    "    # -- 3 --\n",
    "    up8 = concatenate([UpSampling2D()(conv7), conv2], axis=-1)\n",
    "    conv8 = conv_block(up8, 128, \"conv8_1\")\n",
    "    conv8 = conv_block(conv8, 128, \"conv8_2\")\n",
    "    \n",
    "    # -- 4 --\n",
    "    up9 = concatenate([UpSampling2D()(conv8), conv1], axis=-1)\n",
    "    conv9 = conv_block(up9, 64, \"conv9_1\")\n",
    "    conv9 = conv_block(conv9, 64, \"conv9_2\")\n",
    "\n",
    "    vgg = tf.keras.applications.VGG16(input_shape=input_shape, input_tensor=resnet.input, include_top=False)\n",
    "    \n",
    "    # Fix pretrained weights\n",
    "    for l in vgg.layers:\n",
    "        l.trainable = False\n",
    "        \n",
    "    # -- 5 --\n",
    "    vgg_first_conv = vgg.get_layer(\"block1_conv2\").output\n",
    "    up10 = concatenate([UpSampling2D()(conv9), resnet.input, vgg_first_conv], axis=-1)\n",
    "    conv10 = conv_block(up10, 32, \"conv10_1\")\n",
    "    conv10 = conv_block(conv10, 32, \"conv10_2\")\n",
    "    conv10 = SpatialDropout2D(0.2)(conv10)\n",
    "    \n",
    "    \n",
    "    x = Conv2D(1, (1, 1), activation=\"sigmoid\", name=\"prediction\")(conv10)\n",
    "    \n",
    "    # Create Model\n",
    "    model = Model(resnet.input, x)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Intersection over Union metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_IoU(y_true, y_pred):\n",
    "    \n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32) \n",
    "\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    \n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    \n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import RESNET and create the custom NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = tf.keras.applications.ResNet50(input_shape=(img_h, img_w, 3), include_top=False)\n",
    "\n",
    "resnet.summary()\n",
    "\n",
    "\n",
    "model = resnetUnet(resnet, (img_h, img_w, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = tf.keras.losses.BinaryCrossentropy()\n",
    "loss = dice_coef_loss\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "metrics = [my_IoU]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_dataset,\n",
    "          epochs=5,\n",
    "          steps_per_epoch=len(train_img_gen),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=len(valid_img_gen)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "      # Flatten column-wise\n",
    "      pixels = img.T.flatten()\n",
    "      pixels = np.concatenate([[0], pixels, [0]])\n",
    "      runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "      runs[1::2] -= runs[::2]\n",
    "      return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(csv_fname, 'w') as f:\n",
    "\n",
    "      f.write('ImageId,EncodedPixels,Width,Height\\n')\n",
    "\n",
    "      for key, value in results.items():\n",
    "          f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "test_dir = \"/kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset/test\"\n",
    "test_img_dir = os.path.join(test_dir, 'images/img')\n",
    "\n",
    "img_filenames = next(os.walk(test_img_dir))[2]\n",
    "results = {}\n",
    "\n",
    "for image_name in img_filenames:\n",
    "    img = Image.open(os.path.join(test_img_dir,'{}').format(image_name)).convert('RGB')\n",
    "    img = img.resize((img_h, img_w))\n",
    "    img_array = np.array(img)\n",
    "    img_array = np.expand_dims(img_array, 0)\n",
    "    \n",
    "    out = model.predict(x=img_array / 255.)\n",
    "    \n",
    "    out = np.round(out)\n",
    "    \n",
    "    predicted = rle_encode(out)\n",
    "    \n",
    "    name = os.path.splitext(image_name)[0]\n",
    "    \n",
    "    results[name] = predicted\n",
    " \n",
    "create_csv(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
